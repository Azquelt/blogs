---
layout: post
title: "Configuring the Prometheus Alertmanager with Open Liberty"
categories: blog
author_picture: https://avatars3.githubusercontent.com/u/34190173
author_github: https://github.com/jennifer-c
seo-title: Configuring the Prometheus Alertmanager with Open Liberty - OpenLiberty.io
seo-description: Learn how to configure the Prometheus Alertmanager for Open Liberty and MicroProfile Metrics to send alerts via Slack. Alerts help quickly and automatically detect problems for better monitoring and maintenance of your application.
blog_description: "Learn how to configure the Prometheus Alertmanager for Open Liberty and MicroProfile Metrics to send alerts via Slack. Alerts help quickly and automatically detect problems for better monitoring and maintenance of your application."
---
= Configuring Prometheus Alertmanager with Open Liberty
Jennifer Cheng <https://github.com/jennifer-c>

Every application needs a strong monitoring system to catch unexpected issues, whether it's an overloaded heap or a slow-responding servlet. MicroProfile Metrics provides the ability to expose metrics for your application. Used in conjunction with the open-source monitoring system link:https://prometheus.io/[Prometheus] and the link:https://prometheus.io/docs/alerting/overview/[Prometheus Alertmanager], we can build a strong foundation for monitoring your system and reacting quickly to issues.

Before we start, you should have an Open Liberty server with the `mpMetrics-2.0` feature enabled to test your Alertmanager configuration.

Already familiar with Alertmanager? Check out link:https://github.com/jennifer-c/openliberty-alertmanager[a sample configuration], built specifically for Open Liberty. The repository also contains the configuration used as an example in this blog post, if you get stuck along the way.

To learn more about MicroProfile Metrics, check out link:https://openliberty.io/guides/microprofile-metrics.html[this guide.]

== Setting up Prometheus with Open Liberty
. Download link:https://prometheus.io/download/#prometheus[Prometheus] and extract it. (By the way, since you're already there, go ahead and download the Alertmanager as well.)

. Edit the `prometheus.yml` file. Under the `scrape_configs` section, there is already a job configured for Prometheus itself. We need to add one for Open Liberty, which should look something like this:
```
- job_name: 'openliberty'

    scheme: 'https'

    tls_config:
      insecure_skip_verify: true

    static_configs:
      - targets: ['localhost:9443']
```
[start=3]
. Run `./prometheus` (or `prometheus.exe` on Windows). When you see the message `Server is ready to receive web requests`, navigate over to `http://localhost:9090` to view the Prometheus web UI.
. To verify that your setup is working, click on `Targets` under `Status` in the navigation bar. You should see your `openliberty` target with the status `UP`.

image::/img/blog/prometheusAM_ui_status.png[Prometheus Status on UI, align="left"]

== Creating Prometheus Alerting Rules
. Make a file called `alert.yml`. Inside that file, you will be creating alerting rules.
Here's an example:
```
groups:
- name: libertyexample
  rules:
  - alert: heapUsageTooHigh
    expr: base_memory_usedHeap_bytes / base_memory_maxHeap_bytes > 0.9
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "Heap usage is too high"
      description: "{{ $labels.instance }} heap usage is too high"
```
This rule will send an alert called `heapUsageTooHigh` when the PromQL query `base_memory_usedHeap_bytes / base_memory_maxHeap_bytes` is greater than `0.9` (90%) for one minute. (For testing purposes, feel free to change the threshold to something smaller, like 0.05.)

Check out the link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Querying Prometheus] page to learn more about PromQL queries.
[start=2]
. In the `prometheus.yml` file, add the location of your `alert.yml` file under `rule_files`. If it's not in the same directory as Prometheus, make sure you supply the full path.
. Restart Prometheus. Navigate to `http://localhost:9090` and click on `Alerts` on the navigation bar. You should be able to see your alert.

image::/img/blog/prometheusAM_ui_alerts.png[Prometheus Alerts on UI, align="left"]

== Configuring the Prometheus Alertmanager with Slack
Note: you will need a Slack channel to send alerts to, as well as a link:https://api.slack.com/messaging/webhooks[Slack webhook] for that channel.

. Now that Prometheus is set up with rules for our Liberty server, we can set up the Prometheus Alertmanager. If you haven't already downloaded it, you can do so link:https://prometheus.io/download/#prometheus[here.] Extract it.
. Edit `alertmanager.yml` to add your Slack webhook under `global`.
```
  global:
    slack_api_url: <your api url>
```
[start=3]
. Under the `route`, you will see that there is a receiver called `web.hook`. We will be editing this receiver to work with Slack. Right under `webhook_configs`, add another field (aligned with `webhook_configs`) called `slack_configs`. It'll look something like this:
```
slack_configs:
- channel: 'prometheus-alertmanager-test'
  title: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}"
  text: "*Description*: {{ .CommonAnnotations.description }}\n*Severity*: {{ .CommonLabels.severity }}""
```
The `CommonAnnotations` come from your `alert.yml` file. The text is written using the link:https://golang.org/pkg/text/template/[Go templating] system.

[start=4]
. Run `./alertmanager`. The Alertmanager is located at `http://localhost:9093` by default. Since we haven't generated any test data yet, you won't see any alert groups at the moment.

== Receiving Alerts via the Alertmanager
Now that the Alertmanager is set up, we need to configure Prometheus to talk to it.

. In the `prometheus.yml` file inside your Prometheus folder, add `localhost:9093` to the targets under the Alertmanager configuration.
```
# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets:
        - localhost:9093
```
[start=2]
. Restart Prometheus.
. Trigger the alert however you can - with the rule we're using, the easiest way is to change your threshold value to be very low and run your application a few times to use up some of your heap.
. Head over to `http://localhost:9090` and click on Alerts. Your alert should be in the `Pending` or `Firing` state. (Once the alert is in the `Firing` state, you should also be able to see your alert at `http://localhost:9093`.)
. Check your Slack channel to see your message.

image::/img/blog/prometheusAM_slack_alert.png[Alert on Slack, align="left"]

== Using Groups, Routes, and Inhibition
When creating larger alerting systems, it's crucial to keep your alerts organized so that you can respond quickly to any problems. You can configure your Alertmanager to group certain alerts together using _groups_, to send alerts to different locations using _routes_, and to only send useful alerts (while not compromising coverage of your data) with _inhibition_.

If you want to test these configurations out yourself, you'll need to have a couple of rules to play with. To your rule file, `alert.yml`, add the following rules:
```
- alert: heapUsageAbove90%
  expr: base_memory_usedHeap_bytes / base_memory_maxHeap_bytes > 0.9
  for: 30s
  labels:
    alerttype: heap
    severity: critical
  annotations:
    summary: "Heap usage is above 90%"
    description: "{{ $labels.instance }} heap usage above 90%"
- alert: heapUsageAbove50%
  expr: base_memory_usedHeap_bytes / base_memory_maxHeap_bytes > 0.5
  for: 30s
  labels:
    alerttype: heap
    severity: warning
  annotations:
    summary: "Heap usage is above 50%"
    description: "{{ $labels.instance }} heap usage is above 50%"
```
If your `alert.yml` file still has the old rule `heapUsageTooHigh`, you can delete that one. For testing purposes, you can change the thresholds to be much smaller (`0.02` and `0.01`, for example, are what I used to test with.)

=== Routes
There's a time and a place for everything, and that includes alerts. Routing your alerts allows you to use multiple different receivers based on the label assigned to each rule.

For example, if you wanted to use PagerDuty to page critical alerts, and use Slack to send notifications of warning alerts, you can set the `route` to look like the following in `alertmanager.yml`:
```
# The root route. This route is used as the default
# if there are no matches in the child routes.
route:
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 5m
  receiver: 'default_receiver'
  # A child route - all critical alerts follow this route
  # and use the receiver 'pager_receiver'
  routes:
  - match:
      # This can be any label or annotation
      severity: critical
    receiver: pager_receiver
    repeat_interval: 30m
```
And set up a new receiver for PagerDuty by adding this to `receivers`:
```
- name: pager-receiver
  pagerduty_configs:
  - service_key: <your service key>
```

Now, your alerts will be routed to different locations depending on the severity.

=== Groups
If you have a network of systems that goes down, you probably don't want to receive an alert for every single instance - instead, it'd be preferable to get one alert that encapsulates all the other ones.

In your `alertmanager.yml`, under `route`, you can group your alerts by label name:
```
route:
  group_by: [ alerttype ]
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 5m
  receiver: 'default_receiver'
```

The alerts will be grouped by `alerttype`, and the group will only send one alert with all of the information.

image::/img/blog/prometheusAM_alertmanager_grouping.png[Alerts grouped in Alertmanager UI, align="left"]

=== Inhibition
For scenarios where you have multiple alerts that convey the same information, inhibiting your alerts can be useful. For example, if you have one alert that detects when 50% of your memory heap is used, and another alert for 90% of memory heap being used, there's no reason to send out alerts for the 50% problem.

In your `alertmanager.yml`, add the following under `inhibition_rules`:
```
- source_match:
    severity: 'critical'
  # The alert that gets muted
  target_match:
    severity: 'warning'
  # Both source and target need to have the same value to inhibit the alert
  equal: [ 'alerttype' ]
```
The alert that has the label `severity: warning` (the target) will not be sent if there is an alert with the label `severity: critical` (the source). Both alerts must have the same value for the label `alerttype`. In our scenario, the alert `heapUsageAbove50%` will be inhibited if `heapUsageAbove90%` is firing at the same time.

image::/img/blog/prometheusAM_alerts_firing.png[Alerts firing in Prometheus UI, align="left"]

image::/img/blog/prometheusAM_slack_alert_inhibited.png[Slack alert for inhibited alert, align="left"]

If we change the `alerttype` to be different values, the inhibition rule no longer matches, and both alerts will be sent. You can try it out by making the two `alerttype` labels different.

== Silencing Alerts
Sometimes, you need to temporarily stop receiving alerts - for example, if you need to take your server down temporarily for maintenance, you don't want to receive any false positives. To do that, you can silence your alerts in the Alertmanager UI, under the `Silences` tab.

image::/img/blog/prometheusAM_alertmanager_silences.png[Silencing Alerts in Alertmanager UI, align="left"]

The matchers can be any metadata from your rules, e.g. labels, annotations, rule group name, etc.

== Next Steps
Now that we have a basic configuration of the Prometheus Alertmanager set up, we can play with Prometheus rules to create a more comprehensive alerting system. We can also customize our messages to be more informative by using the Go templating system. Need some inspiration? Take a look at a sample configuration for Open Liberty link:https://github.com/jennifer-c/openliberty-alertmanager[here.]
