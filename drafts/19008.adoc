MicroProfile Context Propogation on Open Liberty 19.0.0.8
Yasmin Aumeeruddy
https://github.com/yasmin-aumeeruddy
TO DOOOOOO!!!!!!
Give it a try in link:/about/[Open Liberty] 19.0.0.8.
In Open Liberty 19.0.0.8:

MicroProfile Context Propagation 1.0

OpenID Connect Server

IBM Open Liberty Introspection

The Overlay Cache Introspector

The zip Cache Introspector

If you’re curious about what’s coming in future Open Liberty releases, take a look at our previews in the latest development builds. In particular, get an early insight into MicroProfile Reactive Messaging.

Run your apps using 19.0.0.8
If you’re using Maven, here are the coordinates:

<dependency>
    <groupId>io.openliberty</groupId>
    <artifactId>openliberty-runtime</artifactId>
    <version>19.0.0.8</version>
    <type>zip</type>
</dependency>
Or for Gradle:

dependencies {
    libertyRuntime group: 'io.openliberty', name: 'openliberty-runtime', version: '[19.0.0.8,)'
}
Or if you’re using Docker:

docker pull open-liberty
Or take a look at our Downloads page.

Ask a question on Stack Overflow
MicroProfile Context Propagation 1.0
MicroProfile Context Propagation allows you to create pipelines of dependent stages that run with predictable thread context regardless of which thread the completion stage action ends up running on.

MicroProfile Context Propagation provides completion stages that run with predictable thread context and also benefit from being backed by the automatically-tuned Liberty global thread pool. Configuration of concurrency constraints and context propagation is possible programmatically with a fluent builder pattern where configuration defaults can be specified via MicroProfile Config.

Example in server.xml:

  <featureManager>
    <feature>mpContextPropagation-1.0</feature>
    <feature>cdi-2.0</feature> <!-- used in example -->
    <feature>jndi-1.0</feature> <!-- used in example -->
    ... other features
  </featureManager>
Example usable of programmatic builders:

import org.eclipse.microprofile.context.ManagedExecutor;
import org.eclipse.microprofile.context.ThreadContext;
...

ManagedExecutor executor = ManagedExecutor.builder()
    .maxAsync(5)
    .propagated(ThreadContext.APPLICATION, ThreadContext.SECURITY)
    .build();

CompletableFuture<Integer> stage1 = executor.newIncompleteFuture();
stage1.thenApply(function1).thenAccept(value -> {
    try {
        // access resource reference in application's java:comp namespace,
        DataSource ds = InitialContext.doLookup("java:comp/env/jdbc/ds1");
        ...
    } catch (Exception x) {
        throw new CompletionException(x);
    }
};
...
stage1.complete(result);

...
// Shut down managed executor once the application no longer needs it
executor.shutdown();
Example usage in a CDI bean:

// CDI qualifier which is used to identify the executor instance
@Qualifier
@Retention(RetentionPolicy.RUNTIME)
@Target({ ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER })
public @interface AppContext {}

// Example producer field, defined in a CDI bean,
@Produces @ApplicationScoped @AppContext
ManagedExecutor appContextExecutor = ManagedExecutor.builder()
    .propagated(ThreadContext.APPLICATION)
    .build();

// Example disposer method, also defined in the CDI bean,
void disposeExecutor(@Disposes @AppContext exec) {
    exec.shutdownNow();
}

// Example injection point, defined in a CDI bean,
@Inject @AppContext
ManagedExecutor executor;

...

CompletableFuture<Integer> stage = executor
    .supplyAsync(supplier1)
    .thenApply(function1)
    .thenApplyAsync(value -> {
        try {
            // access resource reference in application's java:comp namespace,
            DataSource ds = InitialContext.doLookup("java:comp/env/jdbc/ds1");
            ...
            return result;
        } catch (Exception x) {
            throw new CompletionException(x);
        }
    });
For more information:

JavaDoc API

Spec binaries and Maven coordinates

Context Propagation 1.0 specification

OpenID Connect Server
OpenID Connect is a technology that allows delegation of user authentication to an external security provider.

Liberty’s OpenID Connect Server feature now includes support for long-lived application passwords and application tokens, for use by non browser applications. Users can acquire these and submit them to a non-browser application, which can then use them to access secured resources on servers configured to accept OAuth access tokens from the OpenID Connect server. The user’s password is never exposed to the non-browser app, and the tokens can be revoked independently if needed. App-passwords are exchanged repetitively by the non-browser app for short lived access tokens using a standard OAuth ROPC flow, so if an access token is ever compromised, it is not valid for long. App-tokens are long-lived access tokens.

Web and REST interfaces for users and administrators are added to administer these tokens.

Create a new Liberty server and use this server.xml. Comments in server.xml explain the new configuration attributes. This server doesn’t have a backing database so is for demo purposes only.

Example in xml:

<server>
    <featureManager>
      <feature>openidConnectServer-1.0</feature>
    </featureManager>

    <openidConnectProvider id="OP" oauthProviderRef="OAuth"
        signatureAlgorithm="RS256" keyStoreRef="defaultKeyStore"
        jwkEnabled="true"
    >
    </openidConnectProvider>

    <!-- internalClientID and internalClientSecret match a defined
         client and are used in creating app-passwords and app-tokens.
         passwordGrantRequiresAppPassword enables the ROPC flow to
         exchange app-passwords for short-lived access tokens.
         appPasswordLifetime and appTokenLifetime set the lifetime of these tokens.
    -->
    <oauthProvider id="OAuth" tokenFormat="mpjwt"
      passwordGrantRequiresAppPassword="true"
      internalClientId="RP"
      internalClientSecret="thesecret"
      appPasswordLifetime="30d"
      >
        <!--
         localStore for demo use, a backing database is used instead in production.
         When localStore is used, all client data and token status is held
         in memory only.
        -->
        <localStore>

          <!-- appPasswordAllowed and appTokenAllowed allow
          this client to create app-passwords and app-tokens -->
          <client displayname="RP" enabled="true"
                name="RP" secret="thesecret"
                scope="openid profile email"
                preAuthorizedScope="openid profile email"
                appPasswordAllowed="true"
                appTokenAllowed="true"
          >
                <redirect>https://localhost:19443/oidcclient/redirect/RP</redirect>
          </client>
        </localStore>
    </oauthProvider>

    <oauth-roles>
        <authenticated>
            <special-subject type="ALL_AUTHENTICATED_USERS" />
        </authenticated>
         <tokenManager>
           <!-- this user can manage the app-passwords and tokens of other users -->
            <user name="admin" />
        </tokenManager>
    </oauth-roles>

    <!-- Basic registry for test / development use. -->
    <basicRegistry id="basic" realm="customRealm">
        <user
          name="admin"
          password="adminpwd" />
        <user
          name="demouser2"
          password="demopassword2" />
         <group name="users">
             <member name="admin"/>
             <member name="demouser2" />
        </group>
    </basicRegistry>

    <httpEndpoint id="defaultHttpEndpoint" host="*" httpPort="29080" httpsPort="29443" />
    <keyStore id="defaultKeyStore" password="keyspass" />

</server>
Now users can request and manage their own tokens at https://host:port/(provider id)/personalTokenManagement. Token admins can administer the tokens of other users at https://host:port/(provider id)/usersTokenManagement.

To try it out, start the server and log in as admin, adminpwd at https://localhost:29443/oidc/endpoint/OP/personalTokenManagement

These tokens can be submitted to non-browser applications, which can in turn use them when accessing secured resources on servers configured to accept access tokens from the OpenID Connect server.

Open Liberty Introspection
When diagnosing problems with a server, the server dump command can be run to obtain information about server configuration, log information, and deployed applications. The command generates a zip archive and Open Liberty 19.0.0.8 introduces two new introspectors to display the data of active root containers on an Open Liberty server process.

Although the server dump command can run on servers that have been stopped or are running, introspector output is produced as a step of running a server dump on a running server only.

Overlay Cache Introspector
The new overlay cache introspector is used to display the active root containers and to display the particular data which has been associated with each container. The viewing archive activity is useful for these purposes:

To see what root containers are active within the Open Liberty process.

To see the on-disk locations of each of the containers.

To obtain a better understanding of typical data which is generated by Open Liberty when starting an application module.

Open Liberty manages application content by creating an overlay container for each of the root locations of the application. An overlay cache is associated with each of the root containers.

The associated overlay cache is a two layer dictionary. Keys for the first tier are relative paths while keys for the second tier are type names. The overlay cache is used as a mechanism to simplify sharing application related data within the Liberty Application Server. Sharing a single overlay container replaces sharing many different data items.

The overlay container introspector output is written to the standard server dump archive as entry to OverlayContainerIntrospector.txt. To create the output, use the following commands:

Start the server:

server start <serverName>
When the server has started, create the dump file:

server dump <serverName>
Each use of the server dump command creates a server dump .zio file which has a name based on the server name and which includes a timestamp in the name.

The overlay cache introspector output has three sections.

Title and timestamp

The descripton of the introspector and overlay container diagnostics are displayed.

Listing of overlay containers

The currently active overlay containers and two containers for each of them, both displayed with a list of URL’s. The two containers and referred to as "Base" and "File" which provide the main and additional storage for the overlay container respectively.

Cache Data for each overlay container

The list of active overlay containers and the contents of their overlay cache is displayed.

Zip Cache Introspector
The new zip cache introspector is used primarily to obtain a view of archive activity within an Open Liberty server process which is useful for a number of purposes:

To understand the basic flow of archive file activity within a Liberty server.

To detect exceptional archive activity, for example, a pattern of frequent opens and closes of archives, or, when an extremely large number of archives are being opened.

To tell if application archives are held open by the Open Liberty process. This is useful for cases where dynamic updates to application files are blocked because the files are locked.

To verify the health of the zip cache layer. For example, to make sure the zip reaper thread (see below) is running and has been closing zip files at scheduled times.

To diagnose the effectiveness of particular zip cache settings.

Introspector output is written to the standard server dump archive as entry to ZipCachingIntrospector.txt.

Use the following commands to create the output: Start the server:

server start <serverName>
When the server has started, create the dump file:

server dump <serverName>
Each use of the server dump command creates a server dump ZIP file which has a name based on the server name and timestamp.

The zip cache introspector output contains the following five sections:

Title and time stamp

The introspector description is displayed with the time and date that the output was created.

List of properties used to configure the zip caching layer

These properties are configuarable as JVM options and in most cases, should be set to their default values.

List of active and recently used zip file handles

The key value for the zip file handles is the full absolute path to an archive file. Information about the cached entries is displayed for each of the file handles.

Top level state of the zip file cache layer

The main function of the cache layer is to delay closing zip files so re-opening zip files that have recently been opened is prevented. The reaper is the thread which eventually closes the zip files and the top level state shows whether the reaper is active, shows the initial, final, and current times of the reaper and includes information about the two threads used by the reaper.

Data for all managed zip files

This data is displayed in four listings: all active and pending zip files, pending zip files which have a short expiration, pending zip files which have a long expiration and lastly, zip files which were recently closed but which have not yet been removed from the cache.

Previews of early implementations available in development builds
You can now also try out early implementations of some new capabilities in the latest Open Liberty development builds:

MicroProfile Reactive Messaging

Testing database connections in Liberty apps with REST APIs

These early implementations are not available in 19.0.0.7 but you can try them out in our daily Docker image by running docker pull openliberty/daily. Let us know what you think!

Reactive messaging in microservices (MicroProfile Reactive Messaging)
An application using reactive messaging is composed of CDI beans consuming, producing, and processing messages passing along reactive streams. These messages can be internal to the application or can be sent and received via different message brokers.

Reactive Messaging provides a very easy to use way to send, receive, and process messages. With MicroProfile Reactive Messaging, you can annotate application beans' methods to have messages on a particular channel (@Incoming, @Outgoing, or both) and Liberty drives those methods appropriately as reactive streams publishers, subscribers, or processors.

To enable the feature include it in your server.xml feature list:

<featureManager>
  <feature>mpReactiveMessaging-1.0</feature>
  ...
</featureManager>
With this feature in the OpenLiberty runtime, an application CDI bean can have one of its methods annotated as being message driven. In the example below, the method processes messages from the "greetings" channel:

@Incoming("greetings")
publicCompletionStage <Void> consume(Message<String> greeting ){
   return greeting.ack();
}
A channel represents a stream of messages of a given type and, usually, the same topic. Channels can operate locally within the process or use message brokers to send messages between services.

For example, with no code changes we could change the consume method above to subscribe to messages from the Kafka greetings topic using a Kafka connector like so:

mp.messaging.incoming.greetings.connector=io.openliberty.kafka
The io.openliberty.kafka connector operates according to the reactive messaging specification. For example the consume method above is, by default, set to consume messages from a Kafka topic queue. Further Kafka client properties can be set for the channel by setting properties that are picked up by the MicroProfile Config specification. For example, System properties via OpenLiberty’s bootstrap.properties file or environment variables from OpenLiberty’s server.env file. As per the reactive messaging specification the following configuration properties are passed to the Kafka client:

mp.messaging.incoming.greetings.[PROPERTY-NAME]=value1
mp.messaging.connector.io.openliberty.kafka.[PROPERTY-NAME]=value2
These are passed to the Kafka Consumer factory method as:

PROPERTY-NAME=value
So, for example, a full set of properties to access IBM Public Cloud Event Streams could look like:

mp.messaging.connector.io.openliberty.kafka.bootstrap.servers=broker-1-eventstreams.cloud.ibm.com:9093,broker-2-eventstreams.cloud.ibm.com:9093
mp.messaging.connector.io.openliberty.kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="token" password="my-apikey";
mp.messaging.connector.io.openliberty.kafka.sasl.mechanism=PLAIN
mp.messaging.connector.io.openliberty.kafka.security.protocol=SASL_SSL
mp.messaging.connector.io.openliberty.kafka.ssl.protocol=TLSv1.2
When using Kafka-based channels, Open Liberty Reactive Messaging 1.0 loads the Kafka client classes using the application classloader. If you are using the io.openliberty.kafka connector to read or write Kafka messages, include in your application a Kafka client API jar that is compatible with your Kafka server. For example, the /WEB-INF/lib/ folder would be a suitable place to place a Kafka client JAR when building the application’s .war file.

This is an early release of the Open Liberty Reactive Messaging Kafka connector. We will look to provide more support for sensible defaults and cloud binding information such as Cloud Foundry’s VCAP_SERVICES environment variable in the 1.0 release.

Find out more in the MicroProfile Reactive Messaging spec.

Testing database connections in Liberty apps with REST APIs
How many times have you had to write a server-side test that gets a connection just to check if your configuration is valid and your app can connect to your database? Now by utilizing the REST API provided by the configValidator-1.0 beta feature, you can validate supported elements of your configuration via REST endpoints.

To enable these REST endpoints, add the configValidator-1.0 beta feature to any server using JDBC, JCA, or JMS technologies. For more information checkout this blog post.

<featureManager>
    <feature>configValidator-1.0</feature>
</featureManager>
